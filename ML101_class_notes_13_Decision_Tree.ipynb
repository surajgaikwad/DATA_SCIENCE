{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd698c57",
      "metadata": {
        "id": "cd698c57"
      },
      "source": [
        "# Decision Tree Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac8f4c8",
      "metadata": {
        "id": "4ac8f4c8"
      },
      "source": [
        "* One of the widely used supervised type machine learning methods for classification and regression is the decision tree algorithm.\n",
        "* It also known as classification and regression tree (CART).\n",
        "* According to predetermined principles, data is constantly divided in this algorithm at each row till the final result is obtained.\n",
        "* Decision trees classify the results into groups until no more similarity is left.\n",
        "* Decision tree is non-parametric approach and does not depend on any probability distribution assumptions.\n",
        "* Decision tree is non-parametric approach and does not depend on any probability distribution assumptions. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d16e1e83",
      "metadata": {
        "id": "d16e1e83"
      },
      "source": [
        "* It is a tree-structured format as shown below:\n",
        "![Decision-Trees-modified-1.png](attachment:Decision-Trees-modified-1.png)\n",
        "source: https://www.xoriant.com/blog/decision-trees-for-classification-a-machine-learning-algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6551b9d5",
      "metadata": {
        "id": "6551b9d5"
      },
      "source": [
        "**The main components of a decision tree are:**\n",
        "\n",
        "- **Root node**: The top most node in a decision tree is known as root node.\n",
        "\n",
        "- **Decision Nodes/Internal node**, which is where the data is split or, say, it is a place for the attribute.\n",
        "\n",
        "- **Leaf Node** which are the final outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb8f301",
      "metadata": {
        "id": "5fb8f301"
      },
      "source": [
        "## Working of a Decision Tree Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45698ffb",
      "metadata": {
        "id": "45698ffb"
      },
      "source": [
        "There are multiple steps:\n",
        "**Stpe 1. Splitting** – It is the splitting of data sets into subgroups. As demonstrated in the figure below, splitting can be done depending on a variety of factors, including class, height, and gender.\n",
        "![DT_split.png](attachment:DT_split.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf808b93",
      "metadata": {
        "id": "cf808b93"
      },
      "source": [
        "Step 2. Pruning – It limits the tree depth by reducing the branches of decision tree.\n",
        "![pruning.png](attachment:pruning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fcde80f",
      "metadata": {
        "id": "0fcde80f"
      },
      "source": [
        "Pruning is further divided into two types:\n",
        "\n",
        "* **Pre-Pruning** – Based on statistically significant associations between attributes and class at any specific nodes, tree stops growing.\n",
        "\n",
        "* **Post-Pruning** – Here validate the performance of the test set model and then based on performance we cut the branches that are a result of overfitting noise from the training set.\n",
        "\n",
        "**3. Tree Selection** – In this step we aim to find the smallest tree that fits the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c05b5cc3",
      "metadata": {
        "id": "c05b5cc3"
      },
      "source": [
        "### Illustration of Constructing a Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f29c89b",
      "metadata": {
        "id": "9f29c89b"
      },
      "source": [
        "**Entropy and Information Gain**\n",
        "\n",
        "* Your data's entropy value indicates how disordered it is.\n",
        "* Entropy is employed in the decision tree since the prime goal of the decision tree is to organize the data by classifying similar data groupings into related categories.\n",
        "* In the below image we have our initial dataset and we applied a decision tree algorithm to compile related data points into a single category.\n",
        "* As is evident from the decision split, the majority of the red circles belong to one class whereas the majority of the blue crosses belong to a different class. Thus, decision tree categorise the traits that may be based on a variety of criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8065cfc8",
      "metadata": {
        "id": "8065cfc8"
      },
      "source": [
        "Let's assume that we have \"N\" sets of the item and that these items fall into two categories. We now use the ratio to categorise the data based on labels:\n",
        "![math_DT1.png](attachment:math_DT1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19741f9b",
      "metadata": {
        "id": "19741f9b"
      },
      "source": [
        "The entropy of our set is given by the following equation:\n",
        "![math_DT2.png](attachment:math_DT2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f29976c",
      "metadata": {
        "id": "7f29976c"
      },
      "source": [
        "Graph for the given formula:\n",
        "![Entropy.png](attachment:Entropy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece9ad53",
      "metadata": {
        "id": "ece9ad53"
      },
      "source": [
        "**Advantages**:\n",
        "\n",
        "Easy to understand and create.\n",
        "Can be applicable for both regression and classification.\n",
        "A robust model with excellent outcomes.\n",
        "Handle large data efficiently.\n",
        "Handle training data well with less effort.\n",
        "\n",
        "**Disadvantages**:\n",
        "\n",
        "**Instability**: Decision tree works well if the information is precise and accurate. A slight change in input may change the tree drastically.\n",
        "\n",
        "**Complexity**: Too many observation and features increases the complexity of the data by increasing the number of branches.\n",
        "\n",
        "**Costs**: Cost is an important factor as it requires good statistical knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d96a892",
      "metadata": {
        "id": "2d96a892"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier # Importing Decision Tree Classifier\n",
        "from sklearn.model_selection import train_test_split # Importing train_test_split function\n",
        "from sklearn import metrics #Importing scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ab92bc",
      "metadata": {
        "id": "80ab92bc",
        "outputId": "88cea3ff-af6a-491c-d77f-0f434a983799"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#importing datasets  \n",
        "pima =pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\")\n",
        "pima.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f304d95",
      "metadata": {
        "id": "7f304d95"
      },
      "source": [
        "#### A proper EDA and feature engineering have to be done before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7468a77e",
      "metadata": {
        "id": "7468a77e",
        "outputId": "40325c18-de1a-4cba-d6c7-1e4a29c27c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  \n",
            "0                     0.627   50  \n",
            "1                     0.351   31  \n",
            "2                     0.672   32  \n",
            "3                     0.167   21  \n",
            "4                     2.288   33  \n",
            "0    1\n",
            "1    0\n",
            "2    1\n",
            "3    0\n",
            "4    1\n",
            "Name: Outcome, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#split dataset in features and target variable\n",
        "#Extracting Independent and dependent Variable  \n",
        "X=pima.iloc[:,:-1]\n",
        "y=pima.iloc[:,-1]\n",
        "print(X.head())\n",
        "print(y.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0a3dfc",
      "metadata": {
        "id": "dd0a3dfc"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=102) # 75% training and 25% test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "044401b0",
      "metadata": {
        "id": "044401b0"
      },
      "outputs": [],
      "source": [
        "# Create Decision Tree classifer object\n",
        "classification = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "classification = classification.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = classification.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ac587e",
      "metadata": {
        "id": "87ac587e",
        "outputId": "2bcc7753-cc39-4ac1-a97c-1ffff9978711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision tree training set accuracy: 1.0000 \n",
            "Decision tree testing set accuracy: 0.7240 \n"
          ]
        }
      ],
      "source": [
        "print(f\"Decision tree training set accuracy: {format(classification.score(X_train, y_train), '.4f')} \")\n",
        "print(f\"Decision tree testing set accuracy: {format(classification.score(X_test, y_test), '.4f')} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323f75f6",
      "metadata": {
        "id": "323f75f6",
        "outputId": "7a758281-9517-4066-9e9e-a320d96644fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.73      0.78       131\n",
            "           1       0.55      0.70      0.62        61\n",
            "\n",
            "    accuracy                           0.72       192\n",
            "   macro avg       0.70      0.72      0.70       192\n",
            "weighted avg       0.75      0.72      0.73       192\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "# print classification report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f197cc",
      "metadata": {
        "id": "90f197cc"
      },
      "outputs": [],
      "source": [
        "# Create Decision Tree classifer object\n",
        "classification = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 3)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "classification = classification.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = classification.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2bb282",
      "metadata": {
        "id": "1d2bb282",
        "outputId": "93ae479f-bf7a-4f37-abd3-cdd543890fb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision tree training set accuracy: 0.7639 \n",
            "Decision tree testing set accuracy: 0.8021 \n"
          ]
        }
      ],
      "source": [
        "print(f\"Decision tree training set accuracy: {format(classification.score(X_train, y_train), '.4f')} \")\n",
        "print(f\"Decision tree testing set accuracy: {format(classification.score(X_test, y_test), '.4f')} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a09dbce",
      "metadata": {
        "id": "1a09dbce",
        "outputId": "a4f1dbb8-bd5b-40f5-d96b-5d0f8452ec7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.84      0.85       131\n",
            "           1       0.68      0.72      0.70        61\n",
            "\n",
            "    accuracy                           0.80       192\n",
            "   macro avg       0.77      0.78      0.78       192\n",
            "weighted avg       0.81      0.80      0.80       192\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "# print classification report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14e9876a",
      "metadata": {
        "id": "14e9876a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}